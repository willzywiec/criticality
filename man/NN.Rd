% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nn.R
\name{NN}
\alias{NN}
\title{NN Function}
\usage{
NN(
  batch.size = 8192,
  code = "mcnp",
  ensemble.size = 5,
  epochs = 1500,
  layers = "8192-256-256-256-256-16",
  loss = "sse",
  opt.alg = "adamax",
  learning.rate = 0.00075,
  val.split = 0.2,
  replot = TRUE,
  verbose = FALSE,
  ext.dir,
  training.dir
)
}
\arguments{
\item{batch.size}{Batch size}

\item{code}{Monte Carlo radiation transport code (e.g., "cog", "mcnp")}

\item{ensemble.size}{Number of deep neural networks in the ensemble}

\item{epochs}{Number of training epochs}

\item{layers}{String that defines the deep neural network architecture (e.g., "64-64")}

\item{loss}{Loss function}

\item{opt.alg}{Optimization algorithm}

\item{learning.rate}{Learning rate}

\item{val.split}{Validation split}

\item{replot}{Boolean (TRUE/FALSE) that determines if plots should be regenerated}

\item{verbose}{Visualize TensorFlow output}

\item{ext.dir}{External directory}

\item{training.dir}{Training directory}
}
\description{
This function ties the Model, Fit, Plot, and Test functions together to build, train, and test a deep neural network metamodel.
}
\examples{
NN(
  batch.size = 128,
  code = "mcnp",
  ensemble.size = 1,
  epochs = 10,
  layers = "8192-256-256-256-256-16",
  loss = "sse",
  opt.alg = "adamax",
  learning.rate = 0.00075,
  val.split = 0.2,
  replot = TRUE,
  verbose = TRUE,
  ext.dir = paste0(.libPaths()[1], "/criticality/data"),
  training.dir = paste0(.libPaths()[1], "/criticality/data")
)
}
